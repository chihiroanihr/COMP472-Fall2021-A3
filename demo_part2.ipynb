{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.display import HTML\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):    \n",
    "    # obtain pre-trained Word2Vec model\n",
    "    print(\"...Model Loading...\")\n",
    "    model = api.load(model_name)\n",
    "    print(\"...Model Loaded...\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(txt_file):\n",
    "    df = pd.DataFrame(columns=['question', 'answer', '0', '1', '2', '3'])\n",
    "\n",
    "    # Load sample_21.txt dataset\n",
    "    file = open(txt_file, 'r')\n",
    "    Lines = file.readlines()\n",
    "\n",
    "    question_guess_dict = {}\n",
    "    df_list = []\n",
    "    ch = 'a'\n",
    "    index = 0\n",
    "    # Strips the newline character\n",
    "    for line in Lines:\n",
    "        line = line.strip().split('.\\t')\n",
    "        if (len(line) == 1):\n",
    "            for key, value in question_guess_dict.items():\n",
    "                if (key == line[0]):\n",
    "                    answer = value\n",
    "                    df_list.append(answer)\n",
    "            for key, value in question_guess_dict.items():\n",
    "                df_list.append(value)\n",
    "            ch = 'a'                    # initialize count  \n",
    "            question_guess_dict.clear() # initialize dict\n",
    "            # append to dataframe\n",
    "            df.loc[index] = df_list\n",
    "            index = index + 1\n",
    "            df_list.clear()\n",
    "        elif (line[0].isnumeric()):\n",
    "            question = line[1]\n",
    "            df_list.append(question)\n",
    "        else:\n",
    "            question_guess_dict[ch] = line[1]\n",
    "            ch = chr(ord(ch) + 1)\n",
    "\n",
    "    # display data frame\n",
    "    HTML(df.to_html())\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def details(df, model):\n",
    "    # new dataframe created to store closest synonyms with the question word \n",
    "    # which is to be found from system model\n",
    "    details_df = pd.DataFrame(columns=['question', 'answer', 'guess', 'label'])\n",
    "\n",
    "    # iterate each data in dataframe by rows\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        # create dictionary to store accuracy value \n",
    "        # for similarity between question-word and each available guess-words\n",
    "        accuracy_dict = {}\n",
    "        \n",
    "        # assign each elements in row to appropriate variables\n",
    "        question = row[\"question\"]\n",
    "        answer = row[\"answer\"]\n",
    "        guess_0 = row[\"0\"]\n",
    "        guess_1 = row[\"1\"]\n",
    "        guess_2 = row[\"2\"]\n",
    "        guess_3 = row[\"3\"]\n",
    "        \n",
    "        # if question-word and answer-word from synonyms.csv found in word2vec model:\n",
    "        if (question in model) and (answer in model):\n",
    "            if guess_0 in model:\n",
    "                accuracy = model.similarity(question, guess_0)  # calculate cosine similarity\n",
    "                accuracy_dict[guess_0] = accuracy               # append both word and accuracy to dictionary\n",
    "            if guess_1 in model:\n",
    "                accuracy = model.similarity(question, guess_1)\n",
    "                accuracy_dict[guess_1] = accuracy\n",
    "            if guess_2 in model:\n",
    "                accuracy = model.similarity(question, guess_2)\n",
    "                accuracy_dict[guess_2] = accuracy\n",
    "            if guess_3 in model:\n",
    "                accuracy = model.similarity(question, guess_3)\n",
    "                accuracy_dict[guess_3] = accuracy\n",
    "            \n",
    "            # sort the accuracy dictionary by keys with higher accuracy value (descending)\n",
    "            sorted_accuracy_tuples = sorted(accuracy_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "            sorted_accuracy_dict = {key: value for key, value in sorted_accuracy_tuples}\n",
    "            #print(sorted_accuracy_dict)\n",
    "            \n",
    "            # obtain the guess word with highest accuracy value (first element in sorted dictionary!)\n",
    "            guess_word = next(iter(sorted_accuracy_dict))\n",
    "            \n",
    "            # check if word guessed by system is equal to the answer in synonyms.csv:\n",
    "            if guess_word == answer:\n",
    "                label = \"correct\"\n",
    "            else:\n",
    "                label = \"wrong\"\n",
    "        \n",
    "        # else if question-word and answer-word not found in word2vec model:\n",
    "        else:\n",
    "            label = \"guess\"\n",
    "            guess_word = \"\"\n",
    "        \n",
    "        # append all the data (question-word, answer-word, guess-word by system, label) to new dataframe\n",
    "        details_df.loc[index] = [question, answer, guess_word, label]\n",
    "\n",
    "    return details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(details_df, model_name):\n",
    "    cwd = os.getcwd()\n",
    "    directory = cwd + \"/outputs/\"\n",
    "\n",
    "    # Create target Directory\n",
    "    try:\n",
    "        os.mkdir(directory)\n",
    "        print(\"Output Directory Created \")\n",
    "    except FileExistsError:\n",
    "        print(\"Output Directory already exists\")\n",
    "\n",
    "    print(\"Files will be stored in directory: \" + directory)\n",
    "        \n",
    "    # save model labels to -details.csv\n",
    "    file_name_details = \"-details.csv\"\n",
    "    full_path_details = directory + model_name + file_name_details\n",
    "    details_df.to_csv(full_path_details, index=False)\n",
    "    print(\"File: \" + file_name_details + \" stored!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Executing Task 2(1): different corpora & same embedding size ========\n",
      "--> 'glove-twitter-200'\n",
      "...Model Loading...\n",
      "...Model Loaded...\n",
      "Output Directory already exists\n",
      "Files will be stored in directory: c:\\Users\\rhina\\OneDrive\\Documents\\GitHub\\COMP472-Fall2021-A3/outputs\n",
      "File: -details.csv stored!\n",
      "--> 'glove-wiki-gigaword-200'\n",
      "...Model Loading...\n",
      "...Model Loaded...\n",
      "Output Directory already exists\n",
      "Files will be stored in directory: c:\\Users\\rhina\\OneDrive\\Documents\\GitHub\\COMP472-Fall2021-A3/outputs\n",
      "File: -details.csv stored!\n",
      "======== Executing Task 2(2): same corpus & different embedding size ========\n",
      "--> 'glove-twitter-25'\n",
      "...Model Loading...\n",
      "...Model Loaded...\n",
      "Output Directory already exists\n",
      "Files will be stored in directory: c:\\Users\\rhina\\OneDrive\\Documents\\GitHub\\COMP472-Fall2021-A3/outputs\n",
      "File: -details.csv stored!\n",
      "--> 'glove-twitter-50'\n",
      "...Model Loading...\n",
      "...Model Loaded...\n",
      "Output Directory already exists\n",
      "Files will be stored in directory: c:\\Users\\rhina\\OneDrive\\Documents\\GitHub\\COMP472-Fall2021-A3/outputs\n",
      "File: -details.csv stored!\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Load \"word2vec-google-news-300\" word2vec pre-trained model\n",
    "# Task 2: Load 4 other English any pre-trained models\n",
    "#   -   2 new models from different corpora but same embedding size\n",
    "#   -   2 new models from the same corpus but different embedding sizes\n",
    "model_names = [ \n",
    "    [\"glove-twitter-200\", \"glove-wiki-gigaword-200\"],\n",
    "    [\"glove-twitter-25\", \"glove-twitter-50\"]\n",
    "]\n",
    "\n",
    "models = []\n",
    "\n",
    "# Load synonyms.csv dataset\n",
    "df = load_csv(\"sample_21.txt\")\n",
    "\n",
    "# Load pre-trained model, compute similarity, and output results to csv files\n",
    "for index, task in enumerate(model_names):\n",
    "    if index == 0:\n",
    "        print(\"======== Executing Task 2(1): different corpora & same embedding size ========\")\n",
    "    else:\n",
    "        print(\"======== Executing Task 2(2): same corpus & different embedding size ========\")\n",
    "    for model_name in task:\n",
    "        print(\"--> '\" + model_name + \"'\")\n",
    "        \n",
    "        # Load pre-trained model\n",
    "        model = load_model(model_name)\n",
    "        models.append(model)\n",
    "\n",
    "        # create details dataframe which computes the \n",
    "        # closest synonym for each word in loaded csv dataset\n",
    "        details_df = details(df, model)\n",
    "        # display new dataframe\n",
    "        HTML(details_df.to_html())\n",
    "\n",
    "        # save both data to csv files\n",
    "        save_csv(details_df, model_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b57f895460dec02687b9e5a92e8158b77d00a27c68f06b1b3c2207406ad9da89"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit ('env64': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
