{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Task 2]    Rhina Kim - 40130779"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all available pre-trained dataset from Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n",
      "glove-twitter-100\n",
      "glove-twitter-200\n",
      "__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "available_models = api.info()['models'].keys()\n",
    "for model in available_models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):    \n",
    "    # obtain pre-trained Word2Vec model\n",
    "    print(\"...Model Loading...\")\n",
    "    model = api.load(model_name)\n",
    "    print(\"...Model Loaded...\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(csv_file):\n",
    "    # Load synonyms.csv dataset\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # display data frame\n",
    "    HTML(df.to_html())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the closest synonym for each word in loaded csv dataset using Similarity method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def details(df, model):\n",
    "    # new dataframe created to store closest synonyms with the question word \n",
    "    # which is to be found from system model\n",
    "    details_df = pd.DataFrame(columns=['question', 'answer', 'guess', 'label'])\n",
    "\n",
    "    # iterate each data in dataframe by rows\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        # create dictionary to store accuracy value \n",
    "        # for similarity between question-word and each available guess-words\n",
    "        accuracy_dict = {}\n",
    "        \n",
    "        # assign each elements in row to appropriate variables\n",
    "        question = row[\"question\"]\n",
    "        answer = row[\"answer\"]\n",
    "        guess_0 = row[\"0\"]\n",
    "        guess_1 = row[\"1\"]\n",
    "        guess_2 = row[\"2\"]\n",
    "        guess_3 = row[\"3\"]\n",
    "        \n",
    "        # if question-word and answer-word from synonyms.csv found in word2vec model:\n",
    "        if (question in model) and (answer in model):\n",
    "            if guess_0 in model:\n",
    "                accuracy = model.similarity(question, guess_0)  # calculate cosine similarity\n",
    "                accuracy_dict[guess_0] = accuracy               # append both word and accuracy to dictionary\n",
    "            if guess_1 in model:\n",
    "                accuracy = model.similarity(question, guess_1)\n",
    "                accuracy_dict[guess_1] = accuracy\n",
    "            if guess_2 in model:\n",
    "                accuracy = model.similarity(question, guess_2)\n",
    "                accuracy_dict[guess_2] = accuracy\n",
    "            if guess_3 in model:\n",
    "                accuracy = model.similarity(question, guess_3)\n",
    "                accuracy_dict[guess_3] = accuracy\n",
    "            \n",
    "            # sort the accuracy dictionary by keys with higher accuracy value (descending)\n",
    "            sorted_accuracy_tuples = sorted(accuracy_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "            sorted_accuracy_dict = {key: value for key, value in sorted_accuracy_tuples}\n",
    "            #print(sorted_accuracy_dict)\n",
    "            \n",
    "            # obtain the guess word with highest accuracy value (first element in sorted dictionary!)\n",
    "            guess_word = next(iter(sorted_accuracy_dict))\n",
    "            \n",
    "            # check if word guessed by system is equal to the answer in synonyms.csv:\n",
    "            if guess_word == answer:\n",
    "                label = \"correct\"\n",
    "            else:\n",
    "                label = \"wrong\"\n",
    "        \n",
    "        # else if question-word and answer-word not found in word2vec model:\n",
    "        else:\n",
    "            label = \"guess\"\n",
    "            guess_word = \"\"\n",
    "        \n",
    "        # append all the data (question-word, answer-word, guess-word by system, label) to new dataframe\n",
    "        details_df.loc[index] = [question, answer, guess_word, label]\n",
    "\n",
    "    return details_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store model info and csv computed dataset insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(details_df, model, model_name):\n",
    "    # new dataframe created to system model analysis\n",
    "    analysis_df = pd.DataFrame(columns=['model_name', 'corpus_size', '#correct_labels', '#questions_answered_no_guess', 'accuracy_model'])\n",
    "\n",
    "    vocab_size = len(model)     # size of the vocabulary(corpus) in model\n",
    "    num_labels_dict = details_df[\"label\"].value_counts()    # returns dictionary with frequency elements in all labels\n",
    "    num_correct_labels = num_labels_dict[\"correct\"]         # number of all elements with correct label\n",
    "    V = num_labels_dict[\"correct\"] + num_labels_dict[\"wrong\"]   # number of all elements with no guess label\n",
    "    accuracy_model_val = num_correct_labels / V             # accuracy model value (C/V)\n",
    "\n",
    "    # append all the data (model_name, corpus_size, #correct_labels, #questions_answered_no_guess, accuracy_model) \n",
    "    # to new dataframe\n",
    "    analysis_df.loc[0] = [model_name, vocab_size, num_correct_labels, V, accuracy_model_val]\n",
    "    \n",
    "    return analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data obtained to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(details_df, analysis_df, model_name):\n",
    "    directory = \"../outputs/\"\n",
    "    \n",
    "    # save model labels to -details.csv\n",
    "    file_name_details = directory + model_name + \"-details.csv\"\n",
    "    details_df.to_csv(file_name_details, index=False)\n",
    "\n",
    "    # save model analysis to -analysis.csv\n",
    "    file_name_analysis = directory + \"analysis.csv\"\n",
    "    analysis_df.to_csv(file_name_analysis, mode='a', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Executing Task 2(1): different corpora & same embedding size ========\n",
      "--> 'glove-twitter-200'\n",
      "...Model Loading...\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Load \"word2vec-google-news-300\" word2vec pre-trained model\n",
    "# Task 2: Load 4 other English any pre-trained models\n",
    "#   -   2 new models from different corpora but same embedding size\n",
    "#   -   2 new models from the same corpus but different embedding sizes\n",
    "model_names = [ \n",
    "    [\"glove-twitter-200\", \"glove-wiki-gigaword-200\"],\n",
    "    [\"glove-twitter-25\", \"glove-twitter-50\"]\n",
    "]\n",
    "\n",
    "# Load synonyms.csv dataset\n",
    "df = load_csv(\"synonyms.csv\")\n",
    "\n",
    "# Load pre-trained model, compute similarity, and output results to csv files\n",
    "for index, task in enumerate(model_names):\n",
    "    if index == 0:\n",
    "        print(\"======== Executing Task 2(1): different corpora & same embedding size ========\")\n",
    "    else:\n",
    "        print(\"======== Executing Task 2(2): same corpus & different embedding size ========\")\n",
    "    for model_name in task:\n",
    "        print(\"--> '\" + model_name + \"'\")\n",
    "        \n",
    "        # Load pre-trained model\n",
    "        model = load_model(model_name)\n",
    "\n",
    "        # create details dataframe which computes the \n",
    "        # closest synonym for each word in loaded csv dataset\n",
    "        details_df = details(df, model)\n",
    "        # display new dataframe\n",
    "        HTML(details_df.to_html())\n",
    "\n",
    "        # Store model info and csv computed dataset insights\n",
    "        analysis_df = analysis(details_df, model, model_name)\n",
    "        # display new dataframe\n",
    "        HTML(analysis_df.to_html())\n",
    "\n",
    "        # save both data to csv files\n",
    "        save_csv(details_df, analysis_df, model_name)\n",
    "\n",
    "# Compare the results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b57f895460dec02687b9e5a92e8158b77d00a27c68f06b1b3c2207406ad9da89"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit ('env64': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
