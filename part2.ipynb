{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.display import HTML\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all available pre-trained dataset from Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n",
      "glove-twitter-100\n",
      "glove-twitter-200\n",
      "__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "available_models = api.info()['models'].keys()\n",
    "for model in available_models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):    \n",
    "    # obtain pre-trained Word2Vec model\n",
    "    print(\"...Model Loading...\")\n",
    "    model = api.load(model_name)\n",
    "    print(\"...Model Loaded...\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(csv_file):\n",
    "    # Load synonyms.csv dataset\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # display data frame\n",
    "    HTML(df.to_html())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the closest synonym for each word in loaded csv dataset using Similarity method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def details(df, model):\n",
    "    # new dataframe created to store closest synonyms with the question word \n",
    "    # which is to be found from system model\n",
    "    details_df = pd.DataFrame(columns=['question', 'answer', 'guess', 'label'])\n",
    "\n",
    "    # iterate each data in dataframe by rows\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        # create dictionary to store accuracy value \n",
    "        # for similarity between question-word and each available guess-words\n",
    "        accuracy_dict = {}\n",
    "        \n",
    "        # assign each elements in row to appropriate variables\n",
    "        question = row[\"question\"]\n",
    "        answer = row[\"answer\"]\n",
    "        guess_0 = row[\"0\"]\n",
    "        guess_1 = row[\"1\"]\n",
    "        guess_2 = row[\"2\"]\n",
    "        guess_3 = row[\"3\"]\n",
    "        \n",
    "        # if question-word and answer-word from synonyms.csv found in word2vec model:\n",
    "        if (question in model) and (answer in model):\n",
    "            if guess_0 in model:\n",
    "                accuracy = model.similarity(question, guess_0)  # calculate cosine similarity\n",
    "                accuracy_dict[guess_0] = accuracy               # append both word and accuracy to dictionary\n",
    "            if guess_1 in model:\n",
    "                accuracy = model.similarity(question, guess_1)\n",
    "                accuracy_dict[guess_1] = accuracy\n",
    "            if guess_2 in model:\n",
    "                accuracy = model.similarity(question, guess_2)\n",
    "                accuracy_dict[guess_2] = accuracy\n",
    "            if guess_3 in model:\n",
    "                accuracy = model.similarity(question, guess_3)\n",
    "                accuracy_dict[guess_3] = accuracy\n",
    "            \n",
    "            # sort the accuracy dictionary by keys with higher accuracy value (descending)\n",
    "            sorted_accuracy_tuples = sorted(accuracy_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "            sorted_accuracy_dict = {key: value for key, value in sorted_accuracy_tuples}\n",
    "            #print(sorted_accuracy_dict)\n",
    "            \n",
    "            # obtain the guess word with highest accuracy value (first element in sorted dictionary!)\n",
    "            guess_word = next(iter(sorted_accuracy_dict))\n",
    "            \n",
    "            # check if word guessed by system is equal to the answer in synonyms.csv:\n",
    "            if guess_word == answer:\n",
    "                label = \"correct\"\n",
    "            else:\n",
    "                label = \"wrong\"\n",
    "        \n",
    "        # else if question-word and answer-word not found in word2vec model:\n",
    "        else:\n",
    "            label = \"guess\"\n",
    "            guess_word = \"\"\n",
    "        \n",
    "        # append all the data (question-word, answer-word, guess-word by system, label) to new dataframe\n",
    "        details_df.loc[index] = [question, answer, guess_word, label]\n",
    "\n",
    "    return details_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data obtained to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(details_df, model_name):\n",
    "    cwd = os.getcwd()\n",
    "    directory = cwd + \"/outputs\"\n",
    "\n",
    "    # Create target Directory\n",
    "    try:\n",
    "        os.mkdir(directory)\n",
    "        print(\"Output Directory Created \")\n",
    "    except FileExistsError:\n",
    "        print(\"Output Directory already exists\")\n",
    "\n",
    "    print(\"Files will be stored in directory: \" + directory)\n",
    "        \n",
    "    # save model labels to -details.csv\n",
    "    file_name_details = \"-details.csv\"\n",
    "    full_path_details = directory + model_name + file_name_details\n",
    "    details_df.to_csv(full_path_details, index=False)\n",
    "    print(\"File: \" + file_name_details + \" stored!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main(): Load & Execute All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Executing Task 2(1): different corpora & same embedding size ========\n",
      "--> 'glove-twitter-200'\n",
      "...Model Loading...\n",
      "...Model Loaded...\n",
      "Output Directory already exists\n",
      "Files will be stored in directory: c:\\Users\\rhina\\OneDrive\\Documents\\GitHub\\COMP472-Fall2021-A3/outputs/\n",
      "File: -details.csv stored!\n",
      "File: analysis.csv stored!\n",
      "--> 'glove-wiki-gigaword-200'\n",
      "...Model Loading...\n",
      "[==================================================] 100.0% 252.1/252.1MB downloaded\n",
      "...Model Loaded...\n",
      "Output Directory already exists\n",
      "Files will be stored in directory: c:\\Users\\rhina\\OneDrive\\Documents\\GitHub\\COMP472-Fall2021-A3/outputs/\n",
      "File: -details.csv stored!\n",
      "File: analysis.csv stored!\n",
      "======== Executing Task 2(2): same corpus & different embedding size ========\n",
      "--> 'glove-twitter-25'\n",
      "...Model Loading...\n",
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
      "...Model Loaded...\n",
      "Output Directory already exists\n",
      "Files will be stored in directory: c:\\Users\\rhina\\OneDrive\\Documents\\GitHub\\COMP472-Fall2021-A3/outputs/\n",
      "File: -details.csv stored!\n",
      "File: analysis.csv stored!\n",
      "--> 'glove-twitter-50'\n",
      "...Model Loading...\n",
      "[==================================================] 100.0% 199.5/199.5MB downloaded\n",
      "...Model Loaded...\n",
      "Output Directory already exists\n",
      "Files will be stored in directory: c:\\Users\\rhina\\OneDrive\\Documents\\GitHub\\COMP472-Fall2021-A3/outputs/\n",
      "File: -details.csv stored!\n",
      "File: analysis.csv stored!\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Load \"word2vec-google-news-300\" word2vec pre-trained model\n",
    "# Task 2: Load 4 other English any pre-trained models\n",
    "#   -   2 new models from different corpora but same embedding size\n",
    "#   -   2 new models from the same corpus but different embedding sizes\n",
    "model_names = [ \n",
    "    [\"glove-twitter-200\", \"glove-wiki-gigaword-200\"],\n",
    "    [\"glove-twitter-25\", \"glove-twitter-50\"]\n",
    "]\n",
    "\n",
    "models = []\n",
    "\n",
    "# Load synonyms.csv dataset\n",
    "df = load_csv(\"synonyms.csv\")\n",
    "\n",
    "# Load pre-trained model, compute similarity, and output results to csv files\n",
    "for index, task in enumerate(model_names):\n",
    "    if index == 0:\n",
    "        print(\"======== Executing Task 2(1): different corpora & same embedding size ========\")\n",
    "    else:\n",
    "        print(\"======== Executing Task 2(2): same corpus & different embedding size ========\")\n",
    "    for model_name in task:\n",
    "        print(\"--> '\" + model_name + \"'\")\n",
    "        \n",
    "        # Load pre-trained model\n",
    "        model = load_model(model_name)\n",
    "        models.append(model)\n",
    "\n",
    "        # create details dataframe which computes the \n",
    "        # closest synonym for each word in loaded csv dataset\n",
    "        details_df = details(df, model)\n",
    "        # display new dataframe\n",
    "        HTML(details_df.to_html())\n",
    "\n",
    "        # save both data to csv files\n",
    "        save_csv(details_df, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Word Vectors using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a 2-dimensional PCA model to the vectors\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b57f895460dec02687b9e5a92e8158b77d00a27c68f06b1b3c2207406ad9da89"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit ('env64': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
